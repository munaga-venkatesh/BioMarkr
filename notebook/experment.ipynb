{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c389ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/venkateshmunaga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/venkateshmunaga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/venkateshmunaga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/venkateshmunaga/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load env vars\n",
    "Entrez.email = os.getenv(\"ENTREZ_EMAIL\")\n",
    "Entrez.api_key = os.getenv(\"ENTREZ_API_KEY\")\n",
    "FETCH_LIMIT = int(os.getenv(\"PUBMED_FETCH_LIMIT\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9de36ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_abstracts(disease, max_articles=400) -> list:\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=disease, retmax=max_articles)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    ids = record[\"IdList\"]\n",
    "\n",
    "    abstracts = []\n",
    "    for pmid in ids:\n",
    "        summary_handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"abstract\", retmode=\"text\")\n",
    "        abstracts.append(summary_handle.read())\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028dd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = fetch_abstracts(disease=\"cancer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_single_token(abstract):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_text = lemmatizer.lemmatize(abstract)\n",
    "    token_text = word_tokenize(lem_text)\n",
    "    stop_words = set(stopwords.words('english'))  # Set for faster lookup\n",
    "    token_text = [w for w in token_text if w.lower() not in stop_words]\n",
    "    return token_text\n",
    "\n",
    "# Main function with multiprocessing\n",
    "def preprocess_abstracts_token(abstracts):\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        results = pool.map(preprocess_single_token, abstracts)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = preprocess_abstracts_token(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_tokens(tokens, entity_dict):\n",
    "    labels = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        matched = False\n",
    "        for length in range(5, 0, -1):  # Try n-gram matches\n",
    "            span = tokens[i:i+length]\n",
    "            phrase = ' '.join(span).lower()\n",
    "            if phrase in entity_dict:\n",
    "                labels.extend([\"B-\"+entity_dict[phrase]] + [\"I-\"+entity_dict[phrase]]*(length-1))\n",
    "                i += length\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            labels.append(\"O\")\n",
    "            i += 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a9e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_conll_format(sentences, labels_list, save_path=\"ner_dataset.conll\"):\n",
    "    with open(save_path, \"w\") as f:\n",
    "        for tokens, labels in zip(sentences, labels_list):\n",
    "            for tok, lab in zip(tokens, labels):\n",
    "                f.write(f\"{tok}\\t{lab}\\n\")\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 2, 3] + [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4e4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
